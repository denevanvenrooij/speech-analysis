{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'features' from '/home/dene/rp2/features.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import functions_praat\n",
    "import features\n",
    "importlib.reload(functions_praat)\n",
    "importlib.reload(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "2025-06-11 15:26:42.847014: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 15:26:43.032067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749648403.098211  785344 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749648403.129800  785344 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749648403.214962  785344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749648403.215023  785344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749648403.215028  785344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749648403.215030  785344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-11 15:26:43.233464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from paths import *\n",
    "from features import *\n",
    "from functions_praat import *\n",
    "from functions_disvoice import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_VOW(audio_file, selected_features, segment_length, f0_min, f0_max, point_step, time_step, max_frequency, num_formants):\n",
    "    extracted_features = {}\n",
    "    if 'PP_F0' in selected_features:\n",
    "        extracted_features['PP_F0'] = PP_f0_mean(audio_file, f0_min=f0_min, f0_max=f0_max)\n",
    "    if 'PP_F0_M' in selected_features:\n",
    "        extracted_features['PP_F0_M'] = PP_f0_mean_murton(audio_file, f0_min=f0_min, f0_max=f0_max)\n",
    "    if 'PP_F0_SD' in selected_features:\n",
    "        extracted_features['PP_F0_SD'] = PP_f0_sd(audio_file, f0_min=f0_min, f0_max=f0_max)\n",
    "    if 'PP_F0_SD_M' in selected_features:\n",
    "        extracted_features['PP_F0_SD_M'] = PP_f0_sd_murton(audio_file, f0_min=f0_min, f0_max=f0_max)\n",
    "\n",
    "    if 'PP_LHR' in selected_features:\n",
    "        extracted_features['PP_LHR'] = PP_lh_ratio(audio_file)\n",
    "    if 'PP_LHR_M' in selected_features:\n",
    "        extracted_features['PP_LHR_M'] = PP_LH_ratio_murton(audio_file, segment_length=segment_length)\n",
    "\n",
    "    if 'PP_CPP_M' in selected_features:\n",
    "        extracted_features['PP_CPP_M'] = PP_CPP_mean_murton(audio_file)\n",
    "    if 'PP_CPP_M2' in selected_features:\n",
    "        extracted_features['PP_CPP_M2'] = PP_CPP_median_murton(audio_file)\n",
    "    if 'PP_CPP_SD_M' in selected_features:\n",
    "        extracted_features['PP_CPP_SD_M'] = PP_CPP_sd_murton(audio_file)\n",
    "\n",
    "    if 'PP_HNR' in selected_features:\n",
    "        extracted_features['PP_HNR'] = PP_harmonics_to_noise(audio_file)\n",
    "    if 'PP_HNR_M' in selected_features:\n",
    "        extracted_features['PP_HNR_M'] = PP_harmonics_to_noise_murton(audio_file, segment_length=segment_length)\n",
    "\n",
    "    if 'PP_JIT' in selected_features:\n",
    "        jitter_values = PP_jitter(audio_file, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in jitter_feature_selection:\n",
    "            if feature in jitter_feature_indices:\n",
    "                feature_idx = jitter_feature_indices[feature]            \n",
    "                extracted_features[f'PP_JIT_{feature}'] = jitter_values[feature_idx]  \n",
    "    if 'PP_JIT_M' in selected_features:\n",
    "        jitter_values = PP_jitter_murton(audio_file, segment_length=segment_length, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in jitter_feature_selection:\n",
    "            if feature in jitter_feature_indices:\n",
    "                feature_idx = jitter_feature_indices[feature]            \n",
    "                extracted_features[f'PP_JIT_M_{feature}'] = jitter_values[feature_idx]    \n",
    "\n",
    "    if 'PP_SHI' in selected_features:\n",
    "        shimmer_values = PP_shimmer(audio_file, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in shimmer_feature_selection:\n",
    "            if feature in shimmer_feature_indices:\n",
    "                feature_idx = shimmer_feature_indices[feature]            \n",
    "                extracted_features[f'PP_SHI_{feature}'] = shimmer_values[feature_idx]\n",
    "    if 'PP_SHI_M' in selected_features:\n",
    "        shimmer_values = PP_shimmer_murton(audio_file, segment_length=segment_length, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in shimmer_feature_selection:\n",
    "            if feature in shimmer_feature_indices:\n",
    "                feature_idx = shimmer_feature_indices[feature]            \n",
    "                extracted_features[f'PP_SHI_M_{feature}'] = shimmer_values[feature_idx]\n",
    "    \n",
    "    if 'PP_GF_MEA' in selected_features:\n",
    "        mean_glottal_formant_values = PP_glottal_formants_mean(audio_file, f0_min=f0_min, f0_max=f0_max, point_step=point_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(mean_glottal_formant_values)): \n",
    "            extracted_features[f'PP_GF{i+1}_MEA'] = mean_glottal_formant_values[i]\n",
    "    if 'PP_GF_MED' in selected_features:\n",
    "        median_glottal_formant_values = PP_glottal_formants_median(audio_file, f0_min=f0_min, f0_max=f0_max, point_step=point_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(median_glottal_formant_values)): \n",
    "            extracted_features[f'PP_GF{i+1}_MED'] = median_glottal_formant_values[i]            \n",
    "    if 'PP_GF_SD' in selected_features:\n",
    "        sd_glottal_formant_values = PP_glottal_formants_sd(audio_file, f0_min=f0_min, f0_max=f0_max, point_step=point_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(sd_glottal_formant_values)): \n",
    "            extracted_features[f'PP_GF{i+1}_SD'] = sd_glottal_formant_values[i]        \n",
    "\n",
    "    if 'PP_F_MEA' in selected_features:\n",
    "        mean_formant_values = PP_formants_mean(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(mean_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_MEA'] = mean_formant_values[i]\n",
    "    if 'PP_F_MED' in selected_features:\n",
    "        median_formant_values = PP_formants_median(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(median_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_MED'] = median_formant_values[i]            \n",
    "    if 'PP_F_SD' in selected_features:\n",
    "        sd_formant_values = PP_formants_sd(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(sd_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_SD'] = sd_formant_values[i]\n",
    "    \n",
    "    if 'DV_PHO' in selected_features: \n",
    "        phonation_features = DV_phonation(audio_file)\n",
    "        for feature in phonation_features.columns:\n",
    "            extracted_features[f'DV_PHO_{feature}'] = phonation_features[feature].values[0]     \n",
    "    if 'DV_GLO' in selected_features:\n",
    "        glottal_features = DV_glottal(audio_file)\n",
    "        for feature in glottal_features.columns:\n",
    "            extracted_features[f'DV_GLO_{feature}'] = glottal_features[feature].values[0]        \n",
    "            \n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_SEN(audio_file, selected_features, f0_min, f0_max, silence_threshold, min_silence_duration, segment_length, num_coefficients, time_step, max_frequency, num_formants):\n",
    "    extracted_features = {}\n",
    "    \n",
    "    if 'PP_F0' in selected_features:\n",
    "        extracted_features['PP_F0'] = PP_f0_median(audio_file, f0_min=f0_min, f0_max=f0_max)    \n",
    "    if 'PP_F0_M' in selected_features:\n",
    "        extracted_features['PP_F0_M'] = PP_f0_median_murton(audio_file, f0_min=f0_min, f0_max=f0_max)  \n",
    "    if 'PP_F0_SD' in selected_features:\n",
    "        extracted_features['PP_F0_SD'] = PP_f0_sd(audio_file, f0_min=f0_min, f0_max=f0_max)\n",
    "    if 'PP_F0_SD_M' in selected_features:\n",
    "        extracted_features['PP_F0_SD_M'] = PP_f0_sd_murton(audio_file, f0_min=f0_min, f0_max=f0_max)\n",
    "        \n",
    "    if 'PP_CPP_M' in selected_features:\n",
    "        extracted_features['PP_CPP_M'] = PP_CPP_mean_murton(audio_file)\n",
    "    if 'PP_CPP_M2' in selected_features:\n",
    "        extracted_features['PP_CPP_M2'] = PP_CPP_median_murton(audio_file)\n",
    "    if 'PP_CPP_SD_M' in selected_features:\n",
    "        extracted_features['PP_CPP_SD_M'] = PP_CPP_sd_murton(audio_file)\n",
    "        \n",
    "    if 'PP_DUR_WP' in selected_features:\n",
    "        extracted_features['PP_DUR_WP'] = PP_duration_with_pauses(audio_file, silence_threshold)\n",
    "    if 'PP_DUR_WOP' in selected_features:\n",
    "        extracted_features['PP_DUR_WOP'] = PP_duration_without_pauses(audio_file, silence_threshold, min_silence_duration)\n",
    "\n",
    "    if 'PP_JIT' in selected_features:\n",
    "        jitter_values = PP_jitter(audio_file, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in jitter_feature_selection:\n",
    "            if feature in jitter_feature_indices:\n",
    "                feature_idx = jitter_feature_indices[feature]            \n",
    "                extracted_features[f'PP_JIT_{feature}'] = jitter_values[feature_idx]  \n",
    "    if 'PP_JIT_M' in selected_features:\n",
    "        jitter_values = PP_jitter_murton(audio_file, segment_length=segment_length, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in jitter_feature_selection:\n",
    "            if feature in jitter_feature_indices:\n",
    "                feature_idx = jitter_feature_indices[feature]            \n",
    "                extracted_features[f'PP_JIT_M_{feature}'] = jitter_values[feature_idx]    \n",
    "                    \n",
    "    if 'PP_SHI' in selected_features:\n",
    "        shimmer_values = PP_shimmer(audio_file, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in shimmer_feature_selection:\n",
    "            if feature in shimmer_feature_indices:\n",
    "                feature_idx = shimmer_feature_indices[feature]            \n",
    "                extracted_features[f'PP_SHI_{feature}'] = shimmer_values[feature_idx]\n",
    "    if 'PP_SHI_M' in selected_features:\n",
    "        shimmer_values = PP_shimmer_murton(audio_file, segment_length=segment_length, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in shimmer_feature_selection:\n",
    "            if feature in shimmer_feature_indices:\n",
    "                feature_idx = shimmer_feature_indices[feature]            \n",
    "                extracted_features[f'PP_SHI_M_{feature}'] = shimmer_values[feature_idx]\n",
    "\n",
    "    if 'PP_MFC' in selected_features:\n",
    "        mfc_values = PP_MFCC(audio_file)\n",
    "        for i in range(num_coefficients):\n",
    "            for feature in mfc_feature_selection:\n",
    "                if feature in mfc_feature_indices:\n",
    "                    feature_idx = mfc_feature_indices[feature]\n",
    "                    extracted_features[f'PP_MFC_{i+1}_{feature}'] = mfc_values[i+(num_coefficients*feature_idx)]\n",
    "\n",
    "    if 'PP_F_MEA' in selected_features:\n",
    "        mean_formant_values = PP_formants_mean(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(mean_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_MEA'] = mean_formant_values[i]\n",
    "    if 'PP_F_MED' in selected_features:\n",
    "        median_formant_values = PP_formants_median(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(median_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_MED'] = median_formant_values[i]            \n",
    "    if 'PP_F_SD' in selected_features:\n",
    "        sd_formant_values = PP_formants_sd(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(sd_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_SD'] = sd_formant_values[i]  \n",
    "\n",
    "    if 'DV_PRO' in selected_features:\n",
    "        prosody_features = DV_prosody(audio_file)\n",
    "        for feature in prosody_features.columns:\n",
    "            extracted_features[f'DV_PRO_{feature}'] = prosody_features[feature].values[0]\n",
    "    if 'DV_PHO' in selected_features: \n",
    "        phonation_features = DV_phonation(audio_file)\n",
    "        for feature in phonation_features.columns:\n",
    "            extracted_features[f'DV_PHO_{feature}'] = phonation_features[feature].values[0]     \n",
    "\n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_SPN(audio_file, selected_features, f0_min, f0_max, silence_threshold, min_silence_duration, segment_length, num_coefficients, time_step, max_frequency, num_formants):\n",
    "    extracted_features = {}\n",
    "    \n",
    "    if 'PP_F0' in selected_features:\n",
    "        extracted_features['PP_F0'] = PP_f0_median(audio_file, f0_min=f0_min, f0_max=f0_max)    \n",
    "    if 'PP_F0_M' in selected_features:\n",
    "        extracted_features['PP_F0_M'] = PP_f0_median_murton(audio_file, f0_min=f0_min, f0_max=f0_max)  \n",
    "    if 'PP_F0_SD' in selected_features:\n",
    "        extracted_features['PP_F0_SD'] = PP_f0_sd(audio_file, f0_min=f0_min, f0_max=f0_max)\n",
    "    if 'PP_F0_SD_M' in selected_features:\n",
    "        extracted_features['PP_F0_SD_M'] = PP_f0_sd_murton(audio_file, f0_min=f0_min, f0_max=f0_max)\n",
    "        \n",
    "    if 'PP_CPP_M' in selected_features:\n",
    "        extracted_features['PP_CPP_M'] = PP_CPP_mean_murton(audio_file)\n",
    "    if 'PP_CPP_M2' in selected_features:\n",
    "        extracted_features['PP_CPP_M2'] = PP_CPP_median_murton(audio_file)\n",
    "    if 'PP_CPP_SD_M' in selected_features:\n",
    "        extracted_features['PP_CPP_SD_M'] = PP_CPP_sd_murton(audio_file)\n",
    "        \n",
    "    if 'PP_DUR_WP' in selected_features:\n",
    "        extracted_features['PP_DUR_WP'] = PP_duration_with_pauses(audio_file, silence_threshold)\n",
    "    if 'PP_DUR_WOP' in selected_features:\n",
    "        extracted_features['PP_DUR_WOP'] = PP_duration_without_pauses(audio_file, silence_threshold, min_silence_duration)\n",
    "\n",
    "    if 'PP_JIT' in selected_features:\n",
    "        jitter_values = PP_jitter(audio_file, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in jitter_feature_selection:\n",
    "            if feature in jitter_feature_indices:\n",
    "                feature_idx = jitter_feature_indices[feature]            \n",
    "                extracted_features[f'PP_JIT_{feature}'] = jitter_values[feature_idx]  \n",
    "    if 'PP_JIT_M' in selected_features:\n",
    "        jitter_values = PP_jitter_murton(audio_file, segment_length=segment_length, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in jitter_feature_selection:\n",
    "            if feature in jitter_feature_indices:\n",
    "                feature_idx = jitter_feature_indices[feature]            \n",
    "                extracted_features[f'PP_JIT_M_{feature}'] = jitter_values[feature_idx]    \n",
    "                    \n",
    "    if 'PP_SHI' in selected_features:\n",
    "        shimmer_values = PP_shimmer(audio_file, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in shimmer_feature_selection:\n",
    "            if feature in shimmer_feature_indices:\n",
    "                feature_idx = shimmer_feature_indices[feature]            \n",
    "                extracted_features[f'PP_SHI_{feature}'] = shimmer_values[feature_idx]\n",
    "    if 'PP_SHI_M' in selected_features:\n",
    "        shimmer_values = PP_shimmer_murton(audio_file, segment_length=segment_length, f0_min=f0_min, f0_max=f0_max) \n",
    "        for feature in shimmer_feature_selection:\n",
    "            if feature in shimmer_feature_indices:\n",
    "                feature_idx = shimmer_feature_indices[feature]            \n",
    "                extracted_features[f'PP_SHI_M_{feature}'] = shimmer_values[feature_idx]\n",
    "\n",
    "    if 'PP_MFC' in selected_features:\n",
    "        mfc_values = PP_MFCC(audio_file)\n",
    "        for i in range(num_coefficients):\n",
    "            for feature in mfc_feature_selection:\n",
    "                if feature in mfc_feature_indices:\n",
    "                    feature_idx = mfc_feature_indices[feature]\n",
    "                    extracted_features[f'PP_MFC_{i+1}_{feature}'] = mfc_values[i+(num_coefficients*feature_idx)]\n",
    "\n",
    "    if 'PP_F_MEA' in selected_features:\n",
    "        mean_formant_values = PP_formants_mean(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(mean_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_MEA'] = mean_formant_values[i]\n",
    "    if 'PP_F_MED' in selected_features:\n",
    "        median_formant_values = PP_formants_median(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(median_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_MED'] = median_formant_values[i]            \n",
    "    if 'PP_F_SD' in selected_features:\n",
    "        sd_formant_values = PP_formants_sd(audio_file, time_step=time_step, max_frequency=max_frequency, num_formants=num_formants)\n",
    "        for i in range(len(sd_formant_values)): \n",
    "            extracted_features[f'PP_F{i+1}_SD'] = sd_formant_values[i]  \n",
    "\n",
    "    if 'DV_PHO' in selected_features: \n",
    "        phonation_features = DV_phonation(audio_file)\n",
    "        for feature in phonation_features.columns:\n",
    "            extracted_features[f'DV_PHO_{feature}'] = phonation_features[feature].values[0]\n",
    "    if 'DV_PRO' in selected_features:\n",
    "        prosody_features = DV_prosody(audio_file)\n",
    "        for feature in prosody_features.columns:\n",
    "            extracted_features[f'DV_PRO_{feature}'] = prosody_features[feature].values[0]\n",
    "    if 'DV_GLO' in selected_features:\n",
    "        glottal_features = DV_glottal(audio_file)\n",
    "        for feature in glottal_features.columns:\n",
    "            extracted_features[f'DV_GLO_{feature}'] = glottal_features[feature].values[0]     \n",
    "            \n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_MPT(audio_file, selected_features, silence_threshold, min_silence_duration):\n",
    "    extracted_features = {}\n",
    "    \n",
    "    if 'PP_DUR_WP' in selected_features:\n",
    "        extracted_features['PP_DUR_WP'] = PP_duration_with_pauses(audio_file, silence_threshold)\n",
    "    if 'PP_DUR_WOP' in selected_features:\n",
    "        extracted_features['PP_DUR_WOP'] = PP_duration_without_pauses(audio_file, silence_threshold, min_silence_duration)\n",
    "            \n",
    "    return extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_files_VOW(paths, selected_features, segment_length, f0_min, f0_max, point_step, time_step, max_frequency, num_formants):\n",
    "    patient_dfs = {}\n",
    "\n",
    "    files = [file for file in paths if file.is_file() and file.suffix == '.wav']\n",
    "\n",
    "    for file in files:\n",
    "        filename = file.stem.replace(\"_pre\", \"\")\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) != 4:\n",
    "            print(f\"Unexpected named audio file: {file}\")\n",
    "            continue\n",
    "\n",
    "        patient_id, day, exercise, take_letter = parts\n",
    "        print(\"Now processing\", parts, \"for VOW\")\n",
    "\n",
    "        features = {}\n",
    "        file_path = str(file)\n",
    "\n",
    "        if exercise == 'VOW':\n",
    "            features = extract_features_VOW(\n",
    "                file_path, selected_features, \n",
    "                segment_length, \n",
    "                f0_min, f0_max,\n",
    "                point_step, time_step, max_frequency, num_formants)\n",
    "        \n",
    "        if features:    \n",
    "            df_entry = {'day': int(day), **features}       \n",
    "            df_key = (patient_id, take_letter, exercise)\n",
    "            if df_key not in patient_dfs:\n",
    "                patient_dfs[df_key] = pd.DataFrame(columns=['day'] + list(features.keys()))\n",
    "            patient_dfs[df_key] = pd.concat([patient_dfs[df_key], pd.DataFrame([df_entry])], ignore_index=True)\n",
    "        \n",
    "        for key, df in patient_dfs.items():\n",
    "            patient_dfs[key] = df.sort_values(by='day', ascending=True).reset_index(drop=True)\n",
    "\n",
    "        for (patient_id, take_letter, exercise), df in patient_dfs.items():\n",
    "            file_name = f\"{patient_id}_{exercise}_{take_letter}.csv\"\n",
    "            file_path = features_dir / exercise / patient_id / file_name\n",
    "\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "def process_audio_files_SEN(paths, selected_features, f0_min, f0_max, silence_threshold, min_silence_duration, segment_length, num_coefficients, time_step, max_frequency, num_formants):\n",
    "    patient_dfs = {}\n",
    "\n",
    "    files = [file for file in paths if file.is_file() and file.suffix == '.wav']\n",
    "\n",
    "    for file in files:\n",
    "        filename = file.stem.replace(\"_pre\", \"\")\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) != 4:\n",
    "            print(f\"Unexpected named audio file: {file}\")\n",
    "            continue\n",
    "\n",
    "        patient_id, day, exercise, take_letter = parts\n",
    "        print(\"Now processing\", parts, \"for SEN\")\n",
    "        \n",
    "        features = {}\n",
    "        file_path = str(file)\n",
    "        \n",
    "        if exercise == 'SEN':\n",
    "            features = extract_features_SEN(\n",
    "                file_path, selected_features, \n",
    "                f0_min, f0_max, \n",
    "                silence_threshold, min_silence_duration, \n",
    "                segment_length,\n",
    "                num_coefficients,\n",
    "                time_step, max_frequency, num_formants)\n",
    "            \n",
    "        if features:\n",
    "            df_entry = {'day': int(day), **features}    \n",
    "            df_key = (patient_id, take_letter, exercise)\n",
    "            if df_key not in patient_dfs:\n",
    "                patient_dfs[df_key] = pd.DataFrame(columns=['day'] + list(features.keys()))\n",
    "            patient_dfs[df_key] = pd.concat([patient_dfs[df_key], pd.DataFrame([df_entry])], ignore_index=True)\n",
    "        \n",
    "        for key, df in patient_dfs.items():\n",
    "            patient_dfs[key] = df.sort_values(by='day', ascending=True).reset_index(drop=True)\n",
    "            \n",
    "        for (patient_id, take_letter, exercise), df in patient_dfs.items():\n",
    "            file_name = f\"{patient_id}_{exercise}_{take_letter}.csv\"\n",
    "            file_path = features_dir / exercise / patient_id / file_name\n",
    "\n",
    "            df.to_csv(file_path, index=False)\n",
    "  \n",
    "\n",
    "def process_audio_files_SPN(paths, selected_features, f0_min, f0_max, silence_threshold, min_silence_duration, segment_length, num_coefficients, time_step, max_frequency, num_formants):\n",
    "    patient_dfs = {}\n",
    "\n",
    "    files = [file for file in paths if file.is_file() and file.suffix == '.wav']\n",
    "\n",
    "    for file in files:\n",
    "        filename = file.stem.replace(\"_pre\", \"\")\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) != 4:\n",
    "            print(f\"Unexpected named audio file: {file}\")\n",
    "            continue\n",
    "\n",
    "        patient_id, day, exercise, take_letter = parts\n",
    "        print(\"Now processing\", parts, \"for SPN\")\n",
    "        \n",
    "        features = {}\n",
    "        file_path = str(file)\n",
    "        \n",
    "        if exercise == 'SPN':\n",
    "            features = extract_features_SPN(\n",
    "                file_path, selected_features, \n",
    "                f0_min, f0_max, \n",
    "                silence_threshold, min_silence_duration, \n",
    "                segment_length,\n",
    "                num_coefficients,\n",
    "                time_step, max_frequency, num_formants)\n",
    "            \n",
    "        if features:\n",
    "            df_entry = {'day': int(day), **features}    \n",
    "            df_key = (patient_id, take_letter, exercise)\n",
    "            if df_key not in patient_dfs:\n",
    "                patient_dfs[df_key] = pd.DataFrame(columns=['day'] + list(features.keys()))\n",
    "            patient_dfs[df_key] = pd.concat([patient_dfs[df_key], pd.DataFrame([df_entry])], ignore_index=True)\n",
    "        \n",
    "        for key, df in patient_dfs.items():\n",
    "            patient_dfs[key] = df.sort_values(by='day', ascending=True).reset_index(drop=True)\n",
    "            \n",
    "        for (patient_id, take_letter, exercise), df in patient_dfs.items():\n",
    "            file_name = f\"{patient_id}_{exercise}_{take_letter}.csv\"\n",
    "            file_path = features_dir / exercise / patient_id / file_name\n",
    "\n",
    "            df.to_csv(file_path, index=False)\n",
    "          \n",
    "\n",
    "def process_audio_files_MPT(paths, selected_features, silence_threshold, min_silence_duration):\n",
    "    patient_dfs = {}\n",
    "\n",
    "    files = [file for file in paths if file.is_file() and file.suffix == '.wav']\n",
    "    \n",
    "    for file in files:\n",
    "        filename = file.stem.replace(\"_pre\", \"\")\n",
    "        parts = filename.split(\"_\")\n",
    "        if len(parts) != 4:\n",
    "            print(f\"Unexpected named audio file: {file}\")\n",
    "            continue\n",
    "\n",
    "        patient_id, day, exercise, take_letter = parts\n",
    "        print(\"Now processing\", parts, \"for MPT\")\n",
    "\n",
    "        features = {}\n",
    "        file_path = str(file)\n",
    "        \n",
    "        if exercise == 'MPT':\n",
    "            features = extract_features_MPT(\n",
    "                file_path, selected_features, \n",
    "                silence_threshold, min_silence_duration)\n",
    "            \n",
    "        if features:            \n",
    "            df_entry = {'day': int(day), **features}    \n",
    "            df_key = (patient_id, take_letter, exercise)\n",
    "            if df_key not in patient_dfs:\n",
    "                patient_dfs[df_key] = pd.DataFrame(columns=['day'] + list(features.keys()))\n",
    "            patient_dfs[df_key] = pd.concat([patient_dfs[df_key], pd.DataFrame([df_entry])], ignore_index=True)\n",
    "            \n",
    "        for key, df in patient_dfs.items():\n",
    "            patient_dfs[key] = df.sort_values(by='day', ascending=True).reset_index(drop=True)\n",
    "\n",
    "        for (patient_id, take_letter, exercise), df in patient_dfs.items():\n",
    "            file_name = f\"{patient_id}_{exercise}_{take_letter}.csv\"\n",
    "            file_path = features_dir / exercise / patient_id / file_name\n",
    "\n",
    "            df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing ['1234571', '4', 'VOW', '3a'] for VOW\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m processed_keys_VOW \u001b[38;5;241m=\u001b[39m {(file\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], file\u001b[38;5;241m.\u001b[39mstem[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m:]) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m processed_files_VOW}\n\u001b[1;32m     13\u001b[0m unprocessed_files_VOW \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m all_files_VOW \u001b[38;5;28;01mif\u001b[39;00m (file\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], file\u001b[38;5;241m.\u001b[39mstem[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_keys_VOW]\n\u001b[0;32m---> 15\u001b[0m \u001b[43mprocess_audio_files_VOW\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munprocessed_files_VOW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselected_features_dict_VOW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf0_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoint_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0025\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_formants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m process_audio_files_SEN(\n\u001b[1;32m     23\u001b[0m     paths\u001b[38;5;241m=\u001b[39munprocessed_files,\n\u001b[1;32m     24\u001b[0m     selected_features\u001b[38;5;241m=\u001b[39mselected_features_dict_SEN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     segment_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     29\u001b[0m     time_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, max_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, num_formants\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     31\u001b[0m process_audio_files_SPN(\n\u001b[1;32m     32\u001b[0m     paths\u001b[38;5;241m=\u001b[39munprocessed_files,\n\u001b[1;32m     33\u001b[0m     selected_features\u001b[38;5;241m=\u001b[39mselected_features_dict_SPN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     segment_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m     38\u001b[0m     time_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, max_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, num_formants\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mprocess_audio_files_VOW\u001b[0;34m(paths, selected_features, segment_length, f0_min, f0_max, point_step, time_step, max_frequency, num_formants)\u001b[0m\n\u001b[1;32m     17\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(file)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exercise \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVOW\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_VOW\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43msegment_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf0_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoint_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_formants\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features:    \n\u001b[1;32m     27\u001b[0m     df_entry \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(day), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfeatures}       \n",
      "Cell \u001b[0;32mIn[3], line 86\u001b[0m, in \u001b[0;36mextract_features_VOW\u001b[0;34m(audio_file, selected_features, segment_length, f0_min, f0_max, point_step, time_step, max_frequency, num_formants)\u001b[0m\n\u001b[1;32m     84\u001b[0m         extracted_features[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDV_PHO_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m phonation_features[feature]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]     \n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDV_GLO\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m selected_features:\n\u001b[0;32m---> 86\u001b[0m     glottal_features \u001b[38;5;241m=\u001b[39m \u001b[43mDV_glottal\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m glottal_features\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     88\u001b[0m         extracted_features[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDV_GLO_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m glottal_features[feature]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]        \n",
      "File \u001b[0;32m~/rp2/functions_disvoice.py:23\u001b[0m, in \u001b[0;36mDV_glottal\u001b[0;34m(audio_file)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mDV_glottal\u001b[39m(audio_file):\n\u001b[1;32m     22\u001b[0m     glottal \u001b[38;5;241m=\u001b[39m Glottal()\n\u001b[0;32m---> 23\u001b[0m     glottal_features \u001b[38;5;241m=\u001b[39m \u001b[43mglottal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataframe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m glottal_features\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/disvoice/glottal/glottal.py:252\u001b[0m, in \u001b[0;36mGlottal.extract_features_file\u001b[0;34m(self, audio, static, plots, fmt, kaldi_file)\u001b[0m\n\u001b[1;32m    249\u001b[0m startf0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    250\u001b[0m stopf0 \u001b[38;5;241m=\u001b[39m sizef0\n\u001b[0;32m--> 252\u001b[0m glottal, g_iaif, GCI \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_glottal_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plots:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_glottal(data_audio, fs, GCI, g_iaif, glottal)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/disvoice/glottal/glottal.py:154\u001b[0m, in \u001b[0;36mGlottal.extract_glottal_signal\u001b[0;34m(self, x, fs)\u001b[0m\n\u001b[1;32m    152\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(x)\n\u001b[1;32m    153\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(x)))\n\u001b[0;32m--> 154\u001b[0m GCIs \u001b[38;5;241m=\u001b[39m \u001b[43mse_vq_varf0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m g_iaif \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[1;32m    156\u001b[0m glottal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(x))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/disvoice/glottal/GCI.py:105\u001b[0m, in \u001b[0;36mse_vq_varf0\u001b[0;34m(x, fs, f0)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#Calculate LP-residual and extract N maxima per mean-based signal determined intervals\u001b[39;00m\n\u001b[1;32m    104\u001b[0m res \u001b[38;5;241m=\u001b[39m GetLPCresidual(x,winLen\u001b[38;5;241m*\u001b[39mfs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m,winShift\u001b[38;5;241m*\u001b[39mfs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m,LPC_ord, VUV_inter) \u001b[38;5;66;03m# Get LP residual\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m MBS \u001b[38;5;241m=\u001b[39m \u001b[43mget_MBS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT0mean\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Extract mean based signal\u001b[39;00m\n\u001b[1;32m    106\u001b[0m interval \u001b[38;5;241m=\u001b[39m get_MBS_GCI_intervals(MBS,fs,T0mean,F0max) \u001b[38;5;66;03m# Define search intervals\u001b[39;00m\n\u001b[1;32m    107\u001b[0m [GCI_N,GCI_relAmp] \u001b[38;5;241m=\u001b[39m search_res_interval_peaks(res,interval,Ncand, VUV_inter) \u001b[38;5;66;03m# Find residual peaks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/disvoice/glottal/utils_gci.py:162\u001b[0m, in \u001b[0;36mget_MBS\u001b[0;34m(x, fs, T0mean)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_MBS\u001b[39m(x,fs,T0mean):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Obtain the mean-based signal\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     MBS\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[0;32m--> 162\u001b[0m     halfL\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1.6\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mT0mean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    164\u001b[0m     StepExp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    165\u001b[0m     Step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mStepExp\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "### processes only files that have not been processed before\n",
    "### this means that once one file, disregarding which 'day' it was recorded on, has been processed\n",
    "### no new files of this same type (id, exercise, mic, vowel) will be processed\n",
    "all_files = [file for folder in ['SPN', 'SEN', 'MPT'] for file in (processed_dir / folder).rglob('*') if file.is_file()]\n",
    "processed_files = [file for folder in ['SPN', 'SEN', 'MPT'] for file in (features_dir / folder).rglob('*') if file.is_file()]\n",
    "processed_keys = {(file.parts[-2], file.stem[-6:]) for file in processed_files}\n",
    "unprocessed_files = [file for file in all_files if (file.parts[-2], file.stem[-10:-4]) not in processed_keys]\n",
    "\n",
    "### this achieves the same but specifically for VOW exercise recordings\n",
    "all_files_VOW = [file for file in (segments_dir / 'VOW').rglob('*') if file.is_file()]\n",
    "processed_files_VOW = [file for file in (features_dir / 'VOW').rglob('*') if file.is_file()]\n",
    "processed_keys_VOW = {(file.parts[-2], file.stem[-6:]) for file in processed_files_VOW}\n",
    "unprocessed_files_VOW = [file for file in all_files_VOW if (file.parts[-2], file.stem[-10:-4]) not in processed_keys_VOW]\n",
    "\n",
    "process_audio_files_VOW(\n",
    "    paths=unprocessed_files_VOW,\n",
    "    selected_features=selected_features_dict_VOW,\n",
    "    segment_length=1.0,\n",
    "    f0_min=60, f0_max=300,\n",
    "    point_step=0.0025, time_step=0.01, max_frequency=5000, num_formants=5)\n",
    "\n",
    "process_audio_files_SEN(\n",
    "    paths=unprocessed_files,\n",
    "    selected_features=selected_features_dict_SEN,\n",
    "    f0_min=60, f0_max=300,\n",
    "    silence_threshold=50, min_silence_duration=0.5,\n",
    "    num_coefficients=12,\n",
    "    segment_length=1.0,\n",
    "    time_step=0.01, max_frequency=5000, num_formants=5)\n",
    "\n",
    "process_audio_files_SPN(\n",
    "    paths=unprocessed_files,\n",
    "    selected_features=selected_features_dict_SPN,\n",
    "    f0_min=60, f0_max=300,\n",
    "    silence_threshold=50, min_silence_duration=0.5,\n",
    "    num_coefficients=12,\n",
    "    segment_length=1.0,\n",
    "    time_step=0.01, max_frequency=5000, num_formants=5)\n",
    "\n",
    "process_audio_files_MPT(\n",
    "    paths=unprocessed_files,\n",
    "    selected_features=selected_features_dict_MPT,\n",
    "    silence_threshold=50, min_silence_duration=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp2",
   "language": "python",
   "name": "rp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
